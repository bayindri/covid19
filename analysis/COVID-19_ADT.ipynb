{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID -19 Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COVID-19_ADT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter LAN ID: abaner02\n",
      "Enter LAN pwd: ········\n"
     ]
    }
   ],
   "source": [
    "#start session\n",
    "import findspark\n",
    "findspark.init('/opt/cloudera/parcels/SPARK2/lib/spark2')\n",
    "#from pyspark import SparkContext\n",
    "#from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from timeit import default_timer as timer\n",
    "#from modules import claims_sql\n",
    "from pyspark.sql.functions import col, round\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "#DW Netezza Login Information\n",
    "import getpass\n",
    "import json\n",
    "from datetime import date, datetime\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None,'display.max_columns', None)\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell #display results setting\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "uid = input(\"Enter LAN ID: \")\n",
    "pwd = getpass.getpass(\"Enter LAN pwd: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"COVID-19_ADT\") \\\n",
    "        .config(\"spark.executor.instances\", \"5\") \\\n",
    "        .config(\"spark.executor.memory\", \"48G\") \\\n",
    "        .config(\"spark.executor.cores\", \"5\") \\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''select * from cln.V_BCBSMA_ADT_CMT_FNL'''\n",
    "df = spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "select distinct \n",
    "encounter_diagnosis,\n",
    "encounter_cheif_complaint,\n",
    "encounter_diagnosis_icd10,\n",
    "encounter_type,\n",
    "encounter_discharge_disposition,\n",
    "encounter_admitted_inpatient,\n",
    "encounter_facility_name,\n",
    "encounter_facility_city,\n",
    "encounter_facility_zip,\n",
    "encounter_account_number,\n",
    "count(distinct member_id)\n",
    "from cln.V_BCBSMA_ADT_CMT_FNL\n",
    "where \n",
    "--encounter_diagnosis like '*COVID*' or encounter_cheif_complaint like '*COVID*'\n",
    "encounter_diagnosis like '%COVID%' or encounter_cheif_complaint like '%COVID%' \n",
    "or encounter_diagnosis like '%covid%' or encounter_cheif_complaint like '%covid%' \n",
    "or encounter_diagnosis like '%CORONAV%' or encounter_cheif_complaint like '%CORONAV%' \n",
    "or encounter_diagnosis like '%coronav%' or encounter_cheif_complaint like '%coronav%' \n",
    "or encounter_diagnosis like '%CORONA V%' or encounter_cheif_complaint like '%CORONA V%' \n",
    "or encounter_diagnosis like '%corona v%' or encounter_cheif_complaint like '%corona v%' \n",
    "group by\n",
    "encounter_diagnosis,\n",
    "encounter_cheif_complaint,\n",
    "encounter_diagnosis_icd10,\n",
    "encounter_type,\n",
    "encounter_discharge_disposition,\n",
    "encounter_admitted_inpatient,\n",
    "encounter_facility_name,\n",
    "encounter_facility_city,\n",
    "encounter_facility_zip,\n",
    "encounter_account_number\n",
    "'''\n",
    "df = spark.sql(sql)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p1 = df.toPandas()\n",
    "df_p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "select distinct \n",
    "encounter_diagnosis\n",
    "from cln.V_BCBSMA_ADT_CMT_FNL\n",
    "\n",
    "'''\n",
    "df_diagnosis = spark.sql(sql).toPandas()\n",
    "df_diagnosis.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "select distinct encounter_account_number from from cln.V_BCBSMA_ADT_CMT_FNL\n",
    "\n",
    "'''\n",
    "acc = spark.sql(sql).toPandas()\n",
    "len(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'java.lang.RuntimeException: java.io.IOException: Permission denied;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o116.load.\n: org.apache.spark.sql.AnalysisException: java.lang.RuntimeException: java.io.IOException: Permission denied;\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:108)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:196)\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1.<init>(HiveSessionStateBuilder.scala:69)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.analyzer(HiveSessionStateBuilder.scala:69)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\n\tat org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:79)\n\tat org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\n\tat org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:428)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:233)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:164)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.RuntimeException: java.io.IOException: Permission denied\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:554)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:180)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:114)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:380)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:282)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:68)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:67)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:197)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:197)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:197)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)\n\t... 31 more\nCaused by: java.io.IOException: Permission denied\n\tat java.io.UnixFileSystem.createFileExclusively(Native Method)\n\tat java.io.File.createTempFile(File.java:2024)\n\tat org.apache.hadoop.hive.common.FileUtils.createTempFile(FileUtils.java:787)\n\tat org.apache.hadoop.hive.ql.session.SessionState.createTempFile(SessionState.java:879)\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:552)\n\t... 46 more\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-76c2005a2472>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"password\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Happy2021'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"driver\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"org.netezza.Driver\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dbtable\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msql\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'java.lang.RuntimeException: java.io.IOException: Permission denied;'"
     ]
    }
   ],
   "source": [
    "sql = '''\n",
    "(select * from V_BCBSMA_ADT_CMT_FNL) a'''\n",
    "\n",
    "#uid = 'abaner02'\n",
    "df = spark \\\n",
    "        .read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", \"jdbc:netezza://bntzp01z.bcbsma.com:5480/PDWAPPRP\") \\\n",
    "        .option(\"user\", 'abaner02') \\\n",
    "        .option(\"password\", 'Happy2021') \\\n",
    "        .option(\"driver\", \"org.netezza.Driver\") \\\n",
    "        .option(\"dbtable\", sql) \\\n",
    "        .load()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123259"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('ADT_DATA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Zip</th>\n",
       "      <th>City</th>\n",
       "      <th>County</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01001</td>\n",
       "      <td>Agawam</td>\n",
       "      <td>Hampden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01002</td>\n",
       "      <td>Amherst</td>\n",
       "      <td>Hampshire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01003</td>\n",
       "      <td>Amherst</td>\n",
       "      <td>Hampshire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01004</td>\n",
       "      <td>Amherst</td>\n",
       "      <td>Hampshire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01005</td>\n",
       "      <td>Barre</td>\n",
       "      <td>Worcester</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Zip     City     County\n",
       "0  01001   Agawam    Hampden\n",
       "1  01002  Amherst  Hampshire\n",
       "2  01003  Amherst  Hampshire\n",
       "3  01004  Amherst  Hampshire\n",
       "4  01005    Barre  Worcester"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load county - zip mapping\n",
    "df_county_zip = pd.read_csv(os.getcwd()+'/input/county_zip_code_list.csv',sep='|',dtype={'Zip':'str'})\n",
    "spark.createDataFrame(df_county_zip).createOrReplaceTempView('COUNTY_ZIP')\n",
    "df_county_zip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "select a.*,\n",
    "    SUBSTRING(a.CURR_TIMESTAMP, 1, 10) AS yyyy_mm_dd,\n",
    "    SUBSTRING(a.CURR_TIMESTAMP, 1, 4) AS yyyy,\n",
    "    SUBSTRING(a.CURR_TIMESTAMP, 1, 7) AS yyyy_mm,\n",
    "    SUBSTRING(a.CURR_TIMESTAMP, 6, 5) AS mm_dd,\n",
    "    b.County as patient_county,\n",
    "    c.County as facility_county\n",
    "    from ADT_DATA as a \n",
    "        left join COUNTY_ZIP b\n",
    "            on a.PATIENT_ZIP_CODE = b.Zip\n",
    "        left join COUNTY_ZIP c\n",
    "            on a.ENCOUNTER_FACILITY_ZIP = c.Zip\n",
    "'''\n",
    "df = spark.sql(sql)\n",
    "df.createOrReplaceTempView('ADT_DATA_COUNTY')\n",
    "df.write.mode(\"overwrite\").format('parquet').saveAsTable('analytics.w5_covid_19_ADT_Netezza_'+uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123259"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = '''\n",
    "select * from ADT_DATA_COUNTY\n",
    "where \n",
    "yyyy = '2020' and ENCOUNTER_ACCOUNT_NUMBER = '04025139'\n",
    "'''\n",
    "df_tjx = spark.sql(sql)\n",
    "df_tjx.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "select \n",
    "    yyyy_mm_dd,mm_dd,--facility_county,\n",
    "    patient_county,count(distinct member_id) as mem_cnt\n",
    "    from ADT_DATA_COUNTY \n",
    "    where yyyy_mm = '2020-03'\n",
    "    group by yyyy_mm_dd,mm_dd,--facility_county,\n",
    "    patient_county \n",
    "    order by 1 DESC\n",
    "'''\n",
    "df_county_grp = spark.sql(sql).toPandas()\n",
    "df_county_grp.to_csv('county_level_ADT_by_date.csv')\n",
    "len(df_county_grp) , df_county_grp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "select \n",
    "    mm_dd,--facility_county,\n",
    "    patient_county,count(distinct member_id) as covid_mem_cnt\n",
    "    from ADT_DATA_COUNTY \n",
    "    where yyyy_mm = '2020-03' and\n",
    "    encounter_diagnosis like '%COVID%' or encounter_cheif_complaint like '%COVID%' \n",
    "    or encounter_diagnosis like '%covid%' or encounter_cheif_complaint like '%covid%' \n",
    "    or encounter_diagnosis like '%CORONAV%' or encounter_cheif_complaint like '%CORONAV%' \n",
    "    or encounter_diagnosis like '%coronav%' or encounter_cheif_complaint like '%coronav%' \n",
    "    or encounter_diagnosis like '%CORONA V%' or encounter_cheif_complaint like '%CORONA V%' \n",
    "    or encounter_diagnosis like '%corona v%' or encounter_cheif_complaint like '%corona v%' \n",
    "    group by mm_dd,--facility_county,\n",
    "    patient_county \n",
    "    order by 1 DESC\n",
    "'''\n",
    "df_county_grp_cvid = spark.sql(sql).toPandas()\n",
    "df_fin = df_county_grp.merge(df_county_grp_cvid, how='left',on=['mm_dd','patient_county']).fillna(0)\n",
    "df_fin.to_csv('county_level_ADT_by_date_covid.csv')\n",
    "len(df_fin) , df_fin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "select distinct\n",
    "    ENCOUNTER_FACILITY_NAME,\n",
    "    ENCOUNTER_FACILITY_STREET,\n",
    "    ENCOUNTER_FACILITY_CITY,\n",
    "    ENCOUNTER_FACILITY_STATE,\n",
    "    ENCOUNTER_FACILITY_ZIP,\n",
    "    facility_county,\n",
    "    yyyy_mm_dd,mm_dd,\n",
    "    count(distinct member_id) as mem_cnt\n",
    "from ADT_DATA_COUNTY\n",
    "   where yyyy = '2020' and ENCOUNTER_FACILITY_STATE = 'MA'\n",
    "   group by\n",
    "   ENCOUNTER_FACILITY_NAME,\n",
    "    ENCOUNTER_FACILITY_STREET,\n",
    "    ENCOUNTER_FACILITY_CITY,\n",
    "    ENCOUNTER_FACILITY_STATE,\n",
    "    ENCOUNTER_FACILITY_ZIP,\n",
    "    facility_county,\n",
    "    yyyy_mm_dd,mm_dd\n",
    "'''\n",
    "df=spark.sql(sql)\n",
    "df.write.mode(\"overwrite\").format('parquet').saveAsTable('analytics.w5_covid_19_ADT_Facilities_'+uid)\n",
    "df.toPandas().to_csv('facilties_county_level_ADT_by_date.csv')\n",
    "len(df.toPandas()), df.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "select distinct \n",
    "encounter_diagnosis,\n",
    "encounter_cheif_complaint,\n",
    "encounter_diagnosis_icd10,\n",
    "encounter_type,\n",
    "encounter_discharge_disposition,\n",
    "encounter_admitted_inpatient,\n",
    "encounter_facility_name,\n",
    "encounter_facility_city,\n",
    "encounter_facility_zip,\n",
    "encounter_account_number,\n",
    "count(distinct member_id)\n",
    "--from cln.V_BCBSMA_ADT_CMT_FNL\n",
    "from ADT_DATA\n",
    "where \n",
    "--encounter_diagnosis like '*COVID*' or encounter_cheif_complaint like '*COVID*'\n",
    "encounter_diagnosis like '%COVID%' or encounter_cheif_complaint like '%COVID%' \n",
    "or encounter_diagnosis like '%covid%' or encounter_cheif_complaint like '%covid%' \n",
    "or encounter_diagnosis like '%CORONAV%' or encounter_cheif_complaint like '%CORONAV%' \n",
    "or encounter_diagnosis like '%coronav%' or encounter_cheif_complaint like '%coronav%' \n",
    "or encounter_diagnosis like '%CORONA V%' or encounter_cheif_complaint like '%CORONA V%' \n",
    "or encounter_diagnosis like '%corona v%' or encounter_cheif_complaint like '%corona v%' \n",
    "group by\n",
    "encounter_diagnosis,\n",
    "encounter_cheif_complaint,\n",
    "encounter_diagnosis_icd10,\n",
    "encounter_type,\n",
    "encounter_discharge_disposition,\n",
    "encounter_admitted_inpatient,\n",
    "encounter_facility_name,\n",
    "encounter_facility_city,\n",
    "encounter_facility_zip,\n",
    "encounter_account_number\n",
    "'''\n",
    "df = spark.sql(sql)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p1_n = df.toPandas()\n",
    "df_p1_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch word count for each abstract\n",
    "#PATIENT_DOB|PATIENT_FULL_ADDRESS|PATIENT_STREET_ADDRESS|PATIENT_CITY|PATIENT_STATE|PATIENT_ZIP_CODE|PATIENT_PHONE_NUMBER|PATIENT_GENDER|          PATIENT_ID|       SUBSCRIBER_ID|PATIENT_DECEASED\n",
    "#DXCG_RISK_SCORE|PMI_RISK_SCORE|IMPACT_PRO_RISK_SCORE|UNABLE_TO_REACH|LANDMARK_ELIGIBLE|AQC_PROVIDER_IDENTIFIER|CASE_MANAGER|EPISODE_STATUS|ENCOUNTER_DISCHARGE_EVENT|TOP_30_DIAGS|TOP_40_DIAGS|BH_DIAGS|MEMBER_AGE|NEXT_OF_KIN|SECURITY_RISK|GAP_IN_CARE|ED_BOARDING_ALERT|SERVICING_PROVIDER_NPI_NUMBER|BILLING_PROVIDER_NPI_NUMBER|EMERGENT_OR_NON_EMERGENT|DEATH_DATE|FILE_YYYYMMDD| LOAD_DATE|\n",
    "\n",
    "cols = ['curr_timestamp',\n",
    "        'member_age','patient_city','patient_state','patient_zip_code','patient_gender','patient_dob',\n",
    "        'encounter_diagnosis','encounter_cheif_complaint',\n",
    "        'encounter_diagnosis_icd10','encounter_type','encounter_discharge_disposition','encounter_discharge_event',\n",
    "        'encounter_admitted_inpatient','encounter_facility_name','encounter_facility_city',\n",
    "        'encounter_facility_zip','encounter_account_number','episode_status',\n",
    "        'TOP_30_DIAGS','TOP_40_DIAGS','BH_DIAGS','DXCG_RISK_SCORE','PMI_RISK_SCORE','IMPACT_PRO_RISK_SCORE',\n",
    "        'SERVICING_PROVIDER_NPI_NUMBER','BILLING_PROVIDER_NPI_NUMBER','LOAD_DATE'\n",
    "       ]\n",
    "df_txt= df.select(*cols).toPandas()\n",
    "df_txt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_txt\n",
    "#https://en.wikipedia.org/wiki/User:Michael_J/County_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "    (select * from ADMIN.V_BCL_MONTHLY_CALL ) a'''\n",
    "#uid = 'abaner02'\n",
    "df = spark \\\n",
    "        .read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", \"jdbc:netezza://bntzp01z.bcbsma.com:5480/PDWAPPRP\") \\\n",
    "        .option(\"user\", uid) \\\n",
    "        .option(\"password\", pwd) \\\n",
    "        .option(\"driver\", \"org.netezza.Driver\") \\\n",
    "        .option(\"dbtable\", sql) \\\n",
    "        .load()\n",
    "\n",
    "df.createOrReplaceTempView('BCL_DATA')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "select \n",
    "    SUBSTRING(a.INTERACTION_DATE, 1, 10) AS yyyy_mm_dd,\n",
    "        SUBSTRING(a.INTERACTION_DATE, 1, 4) AS yyyy,\n",
    "        SUBSTRING(a.INTERACTION_DATE, 1, 7) AS yyyy_mm,\n",
    "        SUBSTRING(a.INTERACTION_DATE, 6, 5) AS mm_dd,\n",
    "        count(distinct MEM_NUM) as mem_cnt\n",
    "        \n",
    "from BCL_DATA a\n",
    "--where SUBSTRING(a.INTERACTION_DATE, 1, 4)  = '2020'\n",
    "group by\n",
    "    SUBSTRING(a.INTERACTION_DATE, 1, 10),\n",
    "        SUBSTRING(a.INTERACTION_DATE, 1, 4),\n",
    "        SUBSTRING(a.INTERACTION_DATE, 1, 7),\n",
    "        SUBSTRING(a.INTERACTION_DATE, 6, 5)\n",
    "\n",
    "'''\n",
    "df = spark.sql(sql).toPandas()\n",
    "len(df),df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df[df['yyyy'] == '2017']), sum(df[df['yyyy'] == '2017']['mem_cnt']))\n",
    "print(len(df[df['yyyy'] == '2018']), sum(df[df['yyyy'] == '2018']['mem_cnt']))\n",
    "print(len(df[df['yyyy'] == '2019']), sum(df[df['yyyy'] == '2019']['mem_cnt']))\n",
    "df[df['yyyy'] == '2019'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "184.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
